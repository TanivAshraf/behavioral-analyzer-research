<!-- Inside public/index.html -->
<script>
    // --- Famous Personalities to Analyze ---

    // --- BEFORE (The Old Code) ---
    /*
    const famousProfiles = [
        { name: 'Elon Musk', url: 'https://twitter.com/elonmusk' },
        { name: 'Satya Nadella', url: 'https://www.linkedin.com/in/satyanadella/' },
        { name: 'Brene Brown', url: 'https://www.instagram.com/brenebrown/' },
        { name: 'Naval Ravikant', url: 'https://twitter.com/naval' },
        { name: 'Gary Vaynerchuk', url: 'https://www.linkedin.com/in/garyvaynerchuk/' }
    ];
    */

    // --- AFTER (The New Code) ---
    const famousProfiles = [
        { name: 'Elon Musk', url: 'https://twitter.com/elonmusk' }
    ];

    let currentAnalysisResult = null; // Store the user's latest analysis

    // --- The rest of the script stays the same ---
</script>```

**Result of this change:** Your application will now only make one request to ScrapingBee and one request to Gemini when the page loads, immediately putting you well under the daily limits.

---

### Solution 2: The Professional Fix - Caching Results

Even with just one profile, there's no reason to re-analyze it every single time someone visits the page. We can analyze it once and then **save the result in our Vercel KV database for 24 hours**. This is called **caching**.

**How it works:**
1.  When a request comes in for Elon Musk, the code will first check our database: "Do I have a recent analysis for Elon Musk?"
2.  If **YES** (a "cache hit"), it will return the saved result instantly without calling ScrapingBee or Gemini. This uses **zero** API credits.
3.  If **NO** (a "cache miss"), it will perform the full analysis like before. But before returning the result, it will save it to the database with a 24-hour expiration.

This is extremely efficient and is the standard practice for building high-performance applications.

#### **Instructions:**

I will provide the full, final code for `api/get-famous.js` that includes this powerful caching logic.

1.  Open the file `api/get-famous.js` on your computer.
2.  **Delete all of its contents** and replace it with the complete code below.

---

### Final Code for `api/get-famous.js` (with Caching)

```javascript
// --- Changelog ---
// 1. Imported the Vercel KV database client.
// 2. Implemented caching logic:
//    - Check for a cached result before doing any work.
//    - If found, return it immediately.
//    - If not found, perform the analysis and then save the new result to the cache with a 24-hour expiration.

import { kv } from '@vercel/kv';

// The retry and getGeminiAnalysis functions remain the same as our last version.
const retry = async (fn, retries = 3, delay = 1000, finalErr = 'Failed after multiple attempts') => {
  try { return await fn(); } catch (err) { if (retries <= 0) { throw new Error(finalErr); } await new Promise(res => setTimeout(res, delay)); return retry(fn, retries - 1, delay * 2, finalErr); }
};

async function getGeminiAnalysis(text) {
    const API_KEY = process.env.GEMINI_API_KEY;
    if (!API_KEY) throw new Error("SERVER CONFIG ERROR: GEMINI_API_KEY is not set.");
    const GEMINI_URL = `https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key=${API_KEY}`;
    const prompt = `You are an expert, empathetic social media analyst... [Your full prompt goes here] ...`;
    const response = await fetch(GEMINI_URL, { method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify({ contents: [{ parts: [{ text: prompt }] }] }) });
    if (!response.ok) { if (response.status === 503 || response.status === 429) { throw new Error(`Temporary Server Error: ${response.status}`); } const errorBody = await response.text(); throw new Error(`Gemini API Error (Status: ${response.status}): ${errorBody}`); }
    const data = await response.json();
    const responseText = data.candidates[0].content.parts[0].text;
    const jsonMatch = responseText.match(/```json\s*([\s\S]*?)\s*```|({[\s\S]*})/);
    if (!jsonMatch) { throw new Error("Could not parse JSON from Gemini response."); }
    return JSON.parse(jsonMatch[1] || jsonMatch[2]);
}

// The main handler, now with caching!
export default async function handler(req, res) {
    const { url, name } = req.query;
    // Create a unique key for the database based on the name.
    const cacheKey = `analysis:${name.replace(/\s+/g, '-').toLowerCase()}`;

    try {
        // --- Step 1: Check the Cache First ---
        let cachedResult = await kv.get(cacheKey);
        if (cachedResult) {
            // Cache Hit! Return the saved data instantly.
            console.log(`CACHE HIT for ${name}`);
            return res.status(200).json({ name, analysis: cachedResult });
        }

        // --- Step 2: If Not in Cache, Run the Analysis (with Retries) ---
        console.log(`CACHE MISS for ${name}. Performing fresh analysis.`);
        const freshResult = await retry(async () => {
            const SCRAPINGBEE_API_KEY = process.env.SCRAPINGBEE_API_KEY;
            if (!url || !name) throw new Error('URL and name parameters are required.');
            if (!SCRAPINGBEE_API_KEY) throw new Error("SERVER CONFIG ERROR: SCRAPINGBEE_API_KEY is not set.");

            const params = new URLSearchParams({ api_key: SCRAPINGBEE_API_KEY, url: url, extract_rules: '{"text":"body"}' });
            const scrapeResponse = await fetch(`https://app.scrapingbee.com/api/v1/?${params.toString()}`);
            if (!scrapeResponse.ok) { if (scrapeResponse.status === 503 || scrapeResponse.status === 429) { throw new Error(`Temporary Scraping Error: ${scrapeResponse.status}`); } const errorText = await scrapeResponse.text(); throw new Error(`Scraping service failed (Status: ${scrapeResponse.status}). Response: ${errorText}`); }
            
            const scrapedData = await scrapeResponse.json();
            const analysis = await getGeminiAnalysis(scrapedData.text.substring(0, 15000));
            return analysis;
        });

        // --- Step 3: Save the Fresh Result to the Cache Before Returning ---
        // We use `setex` which means "set with expiration". 86400 seconds = 24 hours.
        await kv.setex(cacheKey, 86400, freshResult);
        
        res.status(200).json({ name, analysis: freshResult });

    } catch (error) {
        console.error(`[FATAL] Processing ${name} failed after retries:`, error);
        res.status(500).json({ name, error: "An internal server error occurred after multiple attempts.", debug_info: { message: error.message }});
    }
}
